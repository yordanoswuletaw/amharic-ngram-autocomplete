{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re \n",
    "import numpy as np \n",
    "import pandas as pd \n",
    "import matplotlib.pyplot as plt \n",
    "import seaborn as sns\n",
    "from collections import defaultdict\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocess Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1750 250 500\n"
     ]
    }
   ],
   "source": [
    "# read amharic text from a file and return it as a string\n",
    "def load_data(file_path):\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        return f.readlines()\n",
    "\n",
    "def consturct_paragraph(text):\n",
    "    delimiter = '።?'\n",
    "    paragraph = []\n",
    "    sentence = []\n",
    "    for line in text:\n",
    "\n",
    "        word = line.split(' ')[0].strip()\n",
    "        if word:\n",
    "            sentence.append(word)\n",
    "        if word in delimiter:\n",
    "            if len(sentence) > 1:\n",
    "                paragraph.append(sentence)\n",
    "            sentence = []\n",
    "    return paragraph\n",
    "\n",
    "def tokenize_text(text):\n",
    "    cleaned_text = []\n",
    "    for sentence in text:\n",
    "        punctuation_pattern = r\"[.,;()/\\[\\]{}'\\\"<>@#$%^&*_+=|~\\-]\"\n",
    "        cleaned_sentence = re.sub(punctuation_pattern, \"\", sentence)\n",
    "        cleaned_sentence = re.sub(r'\\b[a-zA-Z]+\\d+|\\d+[a-zA-Z]+\\b', '', cleaned_sentence)\n",
    "        cleaned_sentence = re.sub(r'\\b[a-zA-Z]+\\b', '', cleaned_sentence)\n",
    "        cleaned_sentence = re.sub(r\"\\b\\d[\\d,\\.]*\\b\", \"num\", cleaned_sentence)\n",
    "        cleaned_sentence = re.sub(r\"\\s+\", \" \", cleaned_sentence).strip()\n",
    "        cleaned_text.append(cleaned_sentence)\n",
    "    \n",
    "    tokenized_text = []\n",
    "    sentence = []\n",
    "    pattern = r'\\w+|[^\\s\\w]+'\n",
    "    for line in cleaned_text:\n",
    "        for word in re.findall(pattern, line):\n",
    "            delimater = re.findall(r'(፡፡|።|\\?|::)', word)\n",
    "            if delimater:\n",
    "                if len(sentence) > 1:  \n",
    "                    sentence.append(delimater[-1]) \n",
    "                    tokenized_text.append(sentence)\n",
    "                sentence = []\n",
    "            elif word:\n",
    "                sentence.append(word)\n",
    "\n",
    "    return tokenized_text\n",
    "\n",
    "train_data, dev_data, test_data = load_data(\"../data/train.txt\"), load_data(\"../data/dev.txt\"), load_data( \"../data/test.txt\")\n",
    "train_data, dev_data, test_data = consturct_paragraph(train_data), consturct_paragraph(dev_data), consturct_paragraph(test_data)\n",
    "\n",
    "# train_csv, dev_csv, test_csv = load_data(\"../data/train.csv\"), load_data(\"../data/dev.csv\"), load_data(\"../data/test.csv\")\n",
    "# train_csv = tokenize_text(train_csv)\n",
    "# dev_csv = tokenize_text(dev_csv)\n",
    "# test_csv = tokenize_text(test_csv)\n",
    "# train_data += train_csv\n",
    "# dev_data += dev_csv\n",
    "# test_data += test_csv\n",
    "print(len(train_data), len(dev_data), len(test_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(test_csv[::-1])\n",
    "# print(dev_csv)\n",
    "# for i in range(33):\n",
    "#     print(train_csv[len(train_csv) - i - 1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocess Tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_tokens_frequency(tokenized_sentences):\n",
    "    vocabulary = defaultdict(int)\n",
    "    for sentence in tokenized_sentences:\n",
    "        for token in sentence:\n",
    "            vocabulary[token] += 1\n",
    "    \n",
    "    return vocabulary \n",
    "\n",
    "def get_tokens_with_threshold(vocabulary, threshold=2):\n",
    "    tokens = [token for token, freq in vocabulary.items() if freq >= threshold]\n",
    "    return tokens\n",
    "\n",
    "def replace_oov_tokens(tokenized_sentences, closed_vocabulary):\n",
    "    replaced_sentences = []\n",
    "    for sentence in tokenized_sentences:\n",
    "        replaced_sentence = [token if token in closed_vocabulary else '<UNK>' for token in sentence]\n",
    "        replaced_sentences.append(replaced_sentence)\n",
    "    \n",
    "    return replaced_sentences\n",
    "\n",
    "def preprocess_tokens(train_data, dev_data, test_data):\n",
    "    vocabulary = count_tokens_frequency(train_data)\n",
    "    closed_vocabulary = get_tokens_with_threshold(vocabulary, threshold=2)\n",
    "    \n",
    "    train_data = replace_oov_tokens(train_data, closed_vocabulary)\n",
    "    dev_data = replace_oov_tokens(dev_data, closed_vocabulary)\n",
    "    test_data = replace_oov_tokens(test_data, closed_vocabulary)\n",
    "    \n",
    "    return train_data, dev_data, test_data, closed_vocabulary + ['<UNK>']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build N-gram Based Langauge Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_n_grams(tokens, n=1, start_token='<ጀ>', end_token='<ጨ>'):\n",
    "    n_grams = defaultdict(int)\n",
    "    for sentence in tokens:\n",
    "        sentence = [start_token] * (n) + sentence + [end_token] * (n-1)\n",
    "        for i in range(len(sentence) - n + 1):\n",
    "            n_gram = tuple(sentence[i:i+n])\n",
    "            n_grams[n_gram] += 1\n",
    "    \n",
    "    return n_grams\n",
    "\n",
    "def estimate_probability(word, previous_n_gram, n_gram_counts, n_plus1_gram_counts, vocabulary_size, k=1.0):\n",
    "    previous_n_gram = tuple(previous_n_gram)\n",
    "    previous_n_gram_count = n_gram_counts[previous_n_gram]\n",
    "    denominator = previous_n_gram_count + k * vocabulary_size\n",
    "\n",
    "    n_plus1_gram = previous_n_gram + (word,)\n",
    "    n_plus1_gram_count = n_plus1_gram_counts[n_plus1_gram]\n",
    "    \n",
    "    numerator = n_plus1_gram_count + k\n",
    "\n",
    "    probability = numerator / denominator\n",
    "    return probability\n",
    "\n",
    "def estimate_n_gram_probabilities(n_grams, n_gram_counts, n_plus_1_gram_counts, vocabulary, start_token='<ጀ>', end_token='<ጨ>', k=1.0):\n",
    "    probabilities = defaultdict(float)\n",
    "    vocabulary = vocabulary + [end_token]\n",
    "    vocabulary_size = len(vocabulary)\n",
    "    previous_n_gram = tuple(n_grams)\n",
    "\n",
    "    for word in vocabulary:\n",
    "        probability = estimate_probability(word, previous_n_gram, n_gram_counts, n_plus_1_gram_counts, vocabulary_size, k=k)\n",
    "        probabilities[word] = probability\n",
    "    \n",
    "    return probabilities\n",
    "\n",
    "def calculate_perplexity(sentence, n_gram_counts, n_plus1_gram_counts, vocabulary_size, start_token='<ጀ>', end_token='<ጨ>', k=1.0, n=3):\n",
    "    n = n - 1\n",
    "    sentence = [start_token] * n + sentence + [end_token] \n",
    "    sentence = tuple(sentence)\n",
    "    N = len(sentence)\n",
    "    product_pi = 1.0\n",
    "    for t in range(n, N):\n",
    "        n_gram = sentence[t-n:t]\n",
    "        word = sentence[t]\n",
    "        probability = estimate_probability(word, n_gram, n_gram_counts, n_plus1_gram_counts, vocabulary_size, k=k)\n",
    "        product_pi *= 1 / probability\n",
    "    \n",
    "    perplexity = product_pi**(1/N)\n",
    "    return perplexity\n",
    "\n",
    "def average_perplexity(corpus, n_gram_counts, n_plus1_gram_counts, vocabulary_size, start_token='<ጀ>', end_token='<ጨ>', k=1.0, n=3):\n",
    "    total_perplexity = 0\n",
    "    for sentence in corpus:\n",
    "        sentence_perplexity = calculate_perplexity(sentence, n_gram_counts, n_plus1_gram_counts, vocabulary_size, start_token, end_token, k, n)\n",
    "        total_perplexity += sentence_perplexity\n",
    "    \n",
    "    return total_perplexity / len(corpus)\n",
    "\n",
    "def suggest_word(previous_tokens, n_gram_counts, n_plus1_gram_counts, vocabulary, k=1.0, start_token='<ጀ>', end_token='<ጨ>', n=3):\n",
    "    n = n - 1\n",
    "    previous_tokens = [start_token] * n + previous_tokens\n",
    "    previous_n_gram = previous_tokens[-n:]\n",
    "    probabilities = estimate_n_gram_probabilities(previous_n_gram, n_gram_counts, n_plus1_gram_counts, vocabulary, start_token=start_token, end_token=end_token, k=k)\n",
    "    suggestion = max(probabilities, key=probabilities.get)\n",
    "    \n",
    "    return suggestion\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Amharic Langauge Auto-complete with N-gram langauge model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dev Average Perplexity: 219.76437488931504\n",
      "Test Average Perplexity: 202.07280093330664\n",
      "Previous tokens: [], suggestion: <UNK>\n",
      "Previous tokens: ['እንደ'], suggestion: <UNK>\n",
      "Previous tokens: ['እንደ', '<UNK>'], suggestion: <UNK>\n",
      "Previous tokens: ['እንደ', '<UNK>', 'አብዛኞቹ'], suggestion: <UNK>\n",
      "Previous tokens: ['እንደ', '<UNK>', 'አብዛኞቹ', 'የአፍሪቃ'], suggestion: ሃገራት\n",
      "Previous tokens: ['እንደ', '<UNK>', 'አብዛኞቹ', 'የአፍሪቃ', 'ሀገሮች'], suggestion: <UNK>\n",
      "Previous tokens: ['እንደ', '<UNK>', 'አብዛኞቹ', 'የአፍሪቃ', 'ሀገሮች', 'በአብዛኛዉ'], suggestion: የምርጫ\n",
      "Previous tokens: ['እንደ', '<UNK>', 'አብዛኞቹ', 'የአፍሪቃ', 'ሀገሮች', 'በአብዛኛዉ', 'የሥራ'], suggestion: <UNK>\n",
      "Previous tokens: ['እንደ', '<UNK>', 'አብዛኞቹ', 'የአፍሪቃ', 'ሀገሮች', 'በአብዛኛዉ', 'የሥራ', 'ቦታ'], suggestion: <UNK>\n",
      "Previous tokens: ['እንደ', '<UNK>', 'አብዛኞቹ', 'የአፍሪቃ', 'ሀገሮች', 'በአብዛኛዉ', 'የሥራ', 'ቦታ', 'ያለዉ'], suggestion: ።\n",
      "Previous tokens: ['እንደ', '<UNK>', 'አብዛኞቹ', 'የአፍሪቃ', 'ሀገሮች', 'በአብዛኛዉ', 'የሥራ', 'ቦታ', 'ያለዉ', '<UNK>'], suggestion: <UNK>\n",
      "Previous tokens: ['እንደ', '<UNK>', 'አብዛኞቹ', 'የአፍሪቃ', 'ሀገሮች', 'በአብዛኛዉ', 'የሥራ', 'ቦታ', 'ያለዉ', '<UNK>', 'ላይ'], suggestion: <UNK>\n",
      "Previous tokens: [], suggestion: <UNK>\n",
      "Previous tokens: ['ስለዚህም'], suggestion: መረጃ\n",
      "Previous tokens: ['ስለዚህም', 'የጥሬ'], suggestion: ሀብት\n",
      "Previous tokens: ['ስለዚህም', 'የጥሬ', 'ሃብት'], suggestion: የላትም\n",
      "Previous tokens: ['ስለዚህም', 'የጥሬ', 'ሃብት', 'ንግድ'], suggestion: ባንክ\n",
      "Previous tokens: ['ስለዚህም', 'የጥሬ', 'ሃብት', 'ንግድ', '<UNK>'], suggestion: <UNK>\n",
      "Previous tokens: ['ስለዚህም', 'የጥሬ', 'ሃብት', 'ንግድ', '<UNK>', 'ቀዉስ'], suggestion: ነዉ\n",
      "Previous tokens: ['ስለዚህም', 'የጥሬ', 'ሃብት', 'ንግድ', '<UNK>', 'ቀዉስ', '<UNK>'], suggestion: <UNK>\n",
      "Previous tokens: ['ስለዚህም', 'የጥሬ', 'ሃብት', 'ንግድ', '<UNK>', 'ቀዉስ', '<UNK>', 'ወቅት'], suggestion: <UNK>\n",
      "Previous tokens: ['ስለዚህም', 'የጥሬ', 'ሃብት', 'ንግድ', '<UNK>', 'ቀዉስ', '<UNK>', 'ወቅት', 'አብዛኛዉ'], suggestion: ተማሪ\n",
      "Previous tokens: ['ስለዚህም', 'የጥሬ', 'ሃብት', 'ንግድ', '<UNK>', 'ቀዉስ', '<UNK>', 'ወቅት', 'አብዛኛዉ', 'ማኅበረሰብ'], suggestion: በኩል\n",
      "Previous tokens: ['ስለዚህም', 'የጥሬ', 'ሃብት', 'ንግድ', '<UNK>', 'ቀዉስ', '<UNK>', 'ወቅት', 'አብዛኛዉ', 'ማኅበረሰብ', 'ከፍተኛ'], suggestion: <UNK>\n",
      "Previous tokens: ['ስለዚህም', 'የጥሬ', 'ሃብት', 'ንግድ', '<UNK>', 'ቀዉስ', '<UNK>', 'ወቅት', 'አብዛኛዉ', 'ማኅበረሰብ', 'ከፍተኛ', 'ችግር'], suggestion: <UNK>\n",
      "Previous tokens: [], suggestion: <UNK>\n",
      "Previous tokens: ['እንደ'], suggestion: <UNK>\n",
      "Previous tokens: ['እንደ', '<UNK>'], suggestion: <UNK>\n",
      "Previous tokens: ['እንደ', '<UNK>', '<UNK>'], suggestion: <UNK>\n",
      "Previous tokens: ['እንደ', '<UNK>', '<UNK>', 'በነዳጅ'], suggestion: ዘይት\n",
      "Previous tokens: ['እንደ', '<UNK>', '<UNK>', 'በነዳጅ', 'ዘይት'], suggestion: ዋጋ\n",
      "Previous tokens: ['እንደ', '<UNK>', '<UNK>', 'በነዳጅ', 'ዘይት', 'ዘርፍ'], suggestion: <UNK>\n",
      "Previous tokens: ['እንደ', '<UNK>', '<UNK>', 'በነዳጅ', 'ዘይት', 'ዘርፍ', '<UNK>'], suggestion: <UNK>\n",
      "Previous tokens: ['እንደ', '<UNK>', '<UNK>', 'በነዳጅ', 'ዘይት', 'ዘርፍ', '<UNK>', '<UNK>'], suggestion: <UNK>\n",
      "Previous tokens: ['እንደ', '<UNK>', '<UNK>', 'በነዳጅ', 'ዘይት', 'ዘርፍ', '<UNK>', '<UNK>', '<UNK>'], suggestion: <UNK>\n",
      "Previous tokens: ['እንደ', '<UNK>', '<UNK>', 'በነዳጅ', 'ዘይት', 'ዘርፍ', '<UNK>', '<UNK>', '<UNK>', 'ትላልቅ'], suggestion: የአዉሮጳ\n",
      "Previous tokens: ['እንደ', '<UNK>', '<UNK>', 'በነዳጅ', 'ዘይት', 'ዘርፍ', '<UNK>', '<UNK>', '<UNK>', 'ትላልቅ', '<UNK>'], suggestion: <UNK>\n",
      "Previous tokens: ['እንደ', '<UNK>', '<UNK>', 'በነዳጅ', 'ዘይት', 'ዘርፍ', '<UNK>', '<UNK>', '<UNK>', 'ትላልቅ', '<UNK>', 'ትላልቅ'], suggestion: የአዉሮጳ\n",
      "Previous tokens: ['እንደ', '<UNK>', '<UNK>', 'በነዳጅ', 'ዘይት', 'ዘርፍ', '<UNK>', '<UNK>', '<UNK>', 'ትላልቅ', '<UNK>', 'ትላልቅ', '<UNK>'], suggestion: <UNK>\n",
      "Previous tokens: ['እንደ', '<UNK>', '<UNK>', 'በነዳጅ', 'ዘይት', 'ዘርፍ', '<UNK>', '<UNK>', '<UNK>', 'ትላልቅ', '<UNK>', 'ትላልቅ', '<UNK>', 'ላይ'], suggestion: <UNK>\n",
      "Previous tokens: ['እንደ', '<UNK>', '<UNK>', 'በነዳጅ', 'ዘይት', 'ዘርፍ', '<UNK>', '<UNK>', '<UNK>', 'ትላልቅ', '<UNK>', 'ትላልቅ', '<UNK>', 'ላይ', '<UNK>'], suggestion: <UNK>\n",
      "Previous tokens: ['እንደ', '<UNK>', '<UNK>', 'በነዳጅ', 'ዘይት', 'ዘርፍ', '<UNK>', '<UNK>', '<UNK>', 'ትላልቅ', '<UNK>', 'ትላልቅ', '<UNK>', 'ላይ', '<UNK>', 'ቀዉስ'], suggestion: ነዉ\n",
      "Previous tokens: ['እንደ', '<UNK>', '<UNK>', 'በነዳጅ', 'ዘይት', 'ዘርፍ', '<UNK>', '<UNK>', '<UNK>', 'ትላልቅ', '<UNK>', 'ትላልቅ', '<UNK>', 'ላይ', '<UNK>', 'ቀዉስ', 'በጣም'], suggestion: <UNK>\n",
      "Previous tokens: ['እንደ', '<UNK>', '<UNK>', 'በነዳጅ', 'ዘይት', 'ዘርፍ', '<UNK>', '<UNK>', '<UNK>', 'ትላልቅ', '<UNK>', 'ትላልቅ', '<UNK>', 'ላይ', '<UNK>', 'ቀዉስ', 'በጣም', 'ከፍተኛ'], suggestion: <UNK>\n",
      "Previous tokens: ['እንደ', '<UNK>', '<UNK>', 'በነዳጅ', 'ዘይት', 'ዘርፍ', '<UNK>', '<UNK>', '<UNK>', 'ትላልቅ', '<UNK>', 'ትላልቅ', '<UNK>', 'ላይ', '<UNK>', 'ቀዉስ', 'በጣም', 'ከፍተኛ', 'የሚባል'], suggestion: ውሳኔ\n",
      "Previous tokens: [], suggestion: <UNK>\n",
      "Previous tokens: ['እነዚህ'], suggestion: እና\n",
      "Previous tokens: ['እነዚህ', '<UNK>'], suggestion: <UNK>\n",
      "Previous tokens: ['እነዚህ', '<UNK>', 'ሥራ'], suggestion: ላይ\n",
      "Previous tokens: ['እነዚህ', '<UNK>', 'ሥራ', 'ላይ'], suggestion: <UNK>\n",
      "Previous tokens: ['እነዚህ', '<UNK>', 'ሥራ', 'ላይ', '<UNK>'], suggestion: <UNK>\n",
      "Previous tokens: ['እነዚህ', '<UNK>', 'ሥራ', 'ላይ', '<UNK>', 'ገንዘብ'], suggestion: <UNK>\n",
      "Previous tokens: ['እነዚህ', '<UNK>', 'ሥራ', 'ላይ', '<UNK>', 'ገንዘብ', '<UNK>'], suggestion: <UNK>\n",
      "Previous tokens: ['እነዚህ', '<UNK>', 'ሥራ', 'ላይ', '<UNK>', 'ገንዘብ', '<UNK>', 'የሚቆጠር'], suggestion: ነው\n",
      "Previous tokens: ['እነዚህ', '<UNK>', 'ሥራ', 'ላይ', '<UNK>', 'ገንዘብ', '<UNK>', 'የሚቆጠር', 'ቢሆንም'], suggestion: <UNK>\n",
      "Previous tokens: ['እነዚህ', '<UNK>', 'ሥራ', 'ላይ', '<UNK>', 'ገንዘብ', '<UNK>', 'የሚቆጠር', 'ቢሆንም', 'ለ'], suggestion: <UNK>\n",
      "Previous tokens: ['እነዚህ', '<UNK>', 'ሥራ', 'ላይ', '<UNK>', 'ገንዘብ', '<UNK>', 'የሚቆጠር', 'ቢሆንም', 'ለ', 'ዓመት'], suggestion: <UNK>\n",
      "Previous tokens: ['እነዚህ', '<UNK>', 'ሥራ', 'ላይ', '<UNK>', 'ገንዘብ', '<UNK>', 'የሚቆጠር', 'ቢሆንም', 'ለ', 'ዓመት', '<UNK>'], suggestion: <UNK>\n",
      "Previous tokens: ['እነዚህ', '<UNK>', 'ሥራ', 'ላይ', '<UNK>', 'ገንዘብ', '<UNK>', 'የሚቆጠር', 'ቢሆንም', 'ለ', 'ዓመት', '<UNK>', 'የተደረገ'], suggestion: አንድ\n",
      "Previous tokens: [], suggestion: <UNK>\n",
      "Previous tokens: ['ስለዚህ'], suggestion: <UNK>\n",
      "Previous tokens: ['ስለዚህ', '<UNK>'], suggestion: <UNK>\n",
      "Previous tokens: ['ስለዚህ', '<UNK>', 'ግዜ'], suggestion: ወዲህ\n",
      "Previous tokens: ['ስለዚህ', '<UNK>', 'ግዜ', 'መዋለ'], suggestion: ንዋይ\n",
      "Previous tokens: ['ስለዚህ', '<UNK>', 'ግዜ', 'መዋለ', 'ንዋይ'], suggestion: <UNK>\n",
      "Previous tokens: ['ስለዚህ', '<UNK>', 'ግዜ', 'መዋለ', 'ንዋይ', '<UNK>'], suggestion: <UNK>\n",
      "Previous tokens: ['ስለዚህ', '<UNK>', 'ግዜ', 'መዋለ', 'ንዋይ', '<UNK>', 'የልማት'], suggestion: ርዳታ\n",
      "Previous tokens: ['ስለዚህ', '<UNK>', 'ግዜ', 'መዋለ', 'ንዋይ', '<UNK>', 'የልማት', 'ዘርፍ'], suggestion: <UNK>\n",
      "Previous tokens: ['ስለዚህ', '<UNK>', 'ግዜ', 'መዋለ', 'ንዋይ', '<UNK>', 'የልማት', 'ዘርፍ', '<UNK>'], suggestion: <UNK>\n",
      "Previous tokens: ['ስለዚህ', '<UNK>', 'ግዜ', 'መዋለ', 'ንዋይ', '<UNK>', 'የልማት', 'ዘርፍ', '<UNK>', '<UNK>'], suggestion: <UNK>\n",
      "Previous tokens: ['ስለዚህ', '<UNK>', 'ግዜ', 'መዋለ', 'ንዋይ', '<UNK>', 'የልማት', 'ዘርፍ', '<UNK>', '<UNK>', '<UNK>'], suggestion: <UNK>\n",
      "Previous tokens: ['ስለዚህ', '<UNK>', 'ግዜ', 'መዋለ', 'ንዋይ', '<UNK>', 'የልማት', 'ዘርፍ', '<UNK>', '<UNK>', '<UNK>', '<UNK>'], suggestion: <UNK>\n",
      "Previous tokens: ['ስለዚህ', '<UNK>', 'ግዜ', 'መዋለ', 'ንዋይ', '<UNK>', 'የልማት', 'ዘርፍ', '<UNK>', '<UNK>', '<UNK>', '<UNK>', 'ጉዳት'], suggestion: <UNK>\n",
      "Previous tokens: [], suggestion: <UNK>\n",
      "Previous tokens: ['እንደ'], suggestion: <UNK>\n",
      "Previous tokens: ['እንደ', '<UNK>'], suggestion: <UNK>\n",
      "Previous tokens: ['እንደ', '<UNK>', '<UNK>'], suggestion: <UNK>\n",
      "Previous tokens: ['እንደ', '<UNK>', '<UNK>', 'አስተያየት'], suggestion: ደግሞ\n",
      "Previous tokens: ['እንደ', '<UNK>', '<UNK>', 'አስተያየት', 'የአፍሪቃ'], suggestion: ሃገራት\n",
      "Previous tokens: ['እንደ', '<UNK>', '<UNK>', 'አስተያየት', 'የአፍሪቃ', 'ኤኮኖሚ'], suggestion: <UNK>\n",
      "Previous tokens: ['እንደ', '<UNK>', '<UNK>', 'አስተያየት', 'የአፍሪቃ', 'ኤኮኖሚ', 'በቅርቡ'], suggestion: <UNK>\n",
      "Previous tokens: ['እንደ', '<UNK>', '<UNK>', 'አስተያየት', 'የአፍሪቃ', 'ኤኮኖሚ', 'በቅርቡ', '<UNK>'], suggestion: <UNK>\n",
      "Previous tokens: ['እንደ', '<UNK>', '<UNK>', 'አስተያየት', 'የአፍሪቃ', 'ኤኮኖሚ', 'በቅርቡ', '<UNK>', 'የሚል'], suggestion: <UNK>\n",
      "Previous tokens: ['እንደ', '<UNK>', '<UNK>', 'አስተያየት', 'የአፍሪቃ', 'ኤኮኖሚ', 'በቅርቡ', '<UNK>', 'የሚል', 'እምነት'], suggestion: <UNK>\n",
      "Previous tokens: [], suggestion: <UNK>\n",
      "Previous tokens: ['በጀርመን'], suggestion: ላይ\n",
      "Previous tokens: ['በጀርመን', 'ባንክ'], suggestion: <UNK>\n",
      "Previous tokens: ['በጀርመን', 'ባንክ', 'ጥናት'], suggestion: <UNK>\n",
      "Previous tokens: ['በጀርመን', 'ባንክ', 'ጥናት', 'ላይ'], suggestion: <UNK>\n",
      "Previous tokens: ['በጀርመን', 'ባንክ', 'ጥናት', 'ላይ', 'የሚገኙት'], suggestion: እንደ\n",
      "Previous tokens: ['በጀርመን', 'ባንክ', 'ጥናት', 'ላይ', 'የሚገኙት', '<UNK>'], suggestion: <UNK>\n",
      "Previous tokens: ['በጀርመን', 'ባንክ', 'ጥናት', 'ላይ', 'የሚገኙት', '<UNK>', 'እንደሚሉት'], suggestion: <UNK>\n",
      "Previous tokens: ['በጀርመን', 'ባንክ', 'ጥናት', 'ላይ', 'የሚገኙት', '<UNK>', 'እንደሚሉት', '<UNK>'], suggestion: <UNK>\n",
      "Previous tokens: ['በጀርመን', 'ባንክ', 'ጥናት', 'ላይ', 'የሚገኙት', '<UNK>', 'እንደሚሉት', '<UNK>', 'በታች'], suggestion: <UNK>\n",
      "Previous tokens: ['በጀርመን', 'ባንክ', 'ጥናት', 'ላይ', 'የሚገኙት', '<UNK>', 'እንደሚሉት', '<UNK>', 'በታች', 'የሚገኙ'], suggestion: ስደተኞች\n",
      "Previous tokens: ['በጀርመን', 'ባንክ', 'ጥናት', 'ላይ', 'የሚገኙት', '<UNK>', 'እንደሚሉት', '<UNK>', 'በታች', 'የሚገኙ', 'የአፍሪቃ'], suggestion: ሃገራት\n",
      "Previous tokens: ['በጀርመን', 'ባንክ', 'ጥናት', 'ላይ', 'የሚገኙት', '<UNK>', 'እንደሚሉት', '<UNK>', 'በታች', 'የሚገኙ', 'የአፍሪቃ', 'ሃገራት'], suggestion: <UNK>\n",
      "Previous tokens: ['በጀርመን', 'ባንክ', 'ጥናት', 'ላይ', 'የሚገኙት', '<UNK>', 'እንደሚሉት', '<UNK>', 'በታች', 'የሚገኙ', 'የአፍሪቃ', 'ሃገራት', 'በከፍተኛ'], suggestion: ደረጃ\n",
      "Previous tokens: ['በጀርመን', 'ባንክ', 'ጥናት', 'ላይ', 'የሚገኙት', '<UNK>', 'እንደሚሉት', '<UNK>', 'በታች', 'የሚገኙ', 'የአፍሪቃ', 'ሃገራት', 'በከፍተኛ', '<UNK>'], suggestion: <UNK>\n",
      "Previous tokens: ['በጀርመን', 'ባንክ', 'ጥናት', 'ላይ', 'የሚገኙት', '<UNK>', 'እንደሚሉት', '<UNK>', 'በታች', 'የሚገኙ', 'የአፍሪቃ', 'ሃገራት', 'በከፍተኛ', '<UNK>', 'እድገት'], suggestion: እንደ\n",
      "Previous tokens: ['በጀርመን', 'ባንክ', 'ጥናት', 'ላይ', 'የሚገኙት', '<UNK>', 'እንደሚሉት', '<UNK>', 'በታች', 'የሚገኙ', 'የአፍሪቃ', 'ሃገራት', 'በከፍተኛ', '<UNK>', 'እድገት', '<UNK>'], suggestion: <UNK>\n",
      "Previous tokens: [], suggestion: <UNK>\n",
      "Previous tokens: ['በነዚህ'], suggestion: ሀገራት\n",
      "Previous tokens: ['በነዚህ', 'ሃገራት'], suggestion: <UNK>\n",
      "Previous tokens: ['በነዚህ', 'ሃገራት', '<UNK>'], suggestion: <UNK>\n",
      "Previous tokens: ['በነዚህ', 'ሃገራት', '<UNK>', '<UNK>'], suggestion: <UNK>\n",
      "Previous tokens: ['በነዚህ', 'ሃገራት', '<UNK>', '<UNK>', 'ዓ'], suggestion: ም\n",
      "Previous tokens: ['በነዚህ', 'ሃገራት', '<UNK>', '<UNK>', 'ዓ', 'ም'], suggestion: <UNK>\n",
      "Previous tokens: ['በነዚህ', 'ሃገራት', '<UNK>', '<UNK>', 'ዓ', 'ም', 'በመቶ'], suggestion: <UNK>\n",
      "Previous tokens: ['በነዚህ', 'ሃገራት', '<UNK>', '<UNK>', 'ዓ', 'ም', 'በመቶ', 'እድገት'], suggestion: እንደ\n",
      "Previous tokens: ['በነዚህ', 'ሃገራት', '<UNK>', '<UNK>', 'ዓ', 'ም', 'በመቶ', 'እድገት', 'ይታያል'], suggestion: ።\n",
      "Previous tokens: ['በነዚህ', 'ሃገራት', '<UNK>', '<UNK>', 'ዓ', 'ም', 'በመቶ', 'እድገት', 'ይታያል', 'ተብሎ'], suggestion: <UNK>\n",
      "Previous tokens: [], suggestion: <UNK>\n",
      "Previous tokens: ['በሌላ'], suggestion: በኩል\n",
      "Previous tokens: ['በሌላ', 'በኩል'], suggestion: <UNK>\n",
      "Previous tokens: ['በሌላ', 'በኩል', '<UNK>'], suggestion: <UNK>\n",
      "Previous tokens: ['በሌላ', 'በኩል', '<UNK>', 'ሃብት'], suggestion: የላትም\n",
      "Previous tokens: ['በሌላ', 'በኩል', '<UNK>', 'ሃብት', 'ዋጋ'], suggestion: <UNK>\n",
      "Previous tokens: ['በሌላ', 'በኩል', '<UNK>', 'ሃብት', 'ዋጋ', 'ማሽቆልቆል'], suggestion: <UNK>\n",
      "Previous tokens: ['በሌላ', 'በኩል', '<UNK>', 'ሃብት', 'ዋጋ', 'ማሽቆልቆል', 'በአብዛኛዉ'], suggestion: የምርጫ\n",
      "Previous tokens: ['በሌላ', 'በኩል', '<UNK>', 'ሃብት', 'ዋጋ', 'ማሽቆልቆል', 'በአብዛኛዉ', 'ነዋሪ'], suggestion: ።\n",
      "Previous tokens: ['በሌላ', 'በኩል', '<UNK>', 'ሃብት', 'ዋጋ', 'ማሽቆልቆል', 'በአብዛኛዉ', 'ነዋሪ', 'ላይ'], suggestion: <UNK>\n",
      "Previous tokens: ['በሌላ', 'በኩል', '<UNK>', 'ሃብት', 'ዋጋ', 'ማሽቆልቆል', 'በአብዛኛዉ', 'ነዋሪ', 'ላይ', '<UNK>'], suggestion: <UNK>\n",
      "Previous tokens: ['በሌላ', 'በኩል', '<UNK>', 'ሃብት', 'ዋጋ', 'ማሽቆልቆል', 'በአብዛኛዉ', 'ነዋሪ', 'ላይ', '<UNK>', 'ያሳድራል'], suggestion: ።\n",
      "Previous tokens: ['በሌላ', 'በኩል', '<UNK>', 'ሃብት', 'ዋጋ', 'ማሽቆልቆል', 'በአብዛኛዉ', 'ነዋሪ', 'ላይ', '<UNK>', 'ያሳድራል', 'ሲሉ'], suggestion: <UNK>\n",
      "Previous tokens: ['በሌላ', 'በኩል', '<UNK>', 'ሃብት', 'ዋጋ', 'ማሽቆልቆል', 'በአብዛኛዉ', 'ነዋሪ', 'ላይ', '<UNK>', 'ያሳድራል', 'ሲሉ', 'ፕሮፊሰር'], suggestion: ሮበርት\n",
      "Previous tokens: ['በሌላ', 'በኩል', '<UNK>', 'ሃብት', 'ዋጋ', 'ማሽቆልቆል', 'በአብዛኛዉ', 'ነዋሪ', 'ላይ', '<UNK>', 'ያሳድራል', 'ሲሉ', 'ፕሮፊሰር', 'ሮበርት'], suggestion: ካፕል\n",
      "Previous tokens: ['በሌላ', 'በኩል', '<UNK>', 'ሃብት', 'ዋጋ', 'ማሽቆልቆል', 'በአብዛኛዉ', 'ነዋሪ', 'ላይ', '<UNK>', 'ያሳድራል', 'ሲሉ', 'ፕሮፊሰር', 'ሮበርት', 'ካፕል'], suggestion: እንደሚሉት\n",
      "Previous tokens: [], suggestion: <UNK>\n",
      "Previous tokens: ['ይሁን'], suggestion: እንጂ\n",
      "Previous tokens: ['ይሁን', 'እና'], suggestion: <UNK>\n",
      "Previous tokens: ['ይሁን', 'እና', 'ዳግም'], suggestion: <UNK>\n",
      "Previous tokens: ['ይሁን', 'እና', 'ዳግም', '<UNK>'], suggestion: <UNK>\n",
      "Previous tokens: ['ይሁን', 'እና', 'ዳግም', '<UNK>', 'ሰዎች'], suggestion: <UNK>\n",
      "Previous tokens: ['ይሁን', 'እና', 'ዳግም', '<UNK>', 'ሰዎች', '<UNK>'], suggestion: <UNK>\n",
      "Previous tokens: ['ይሁን', 'እና', 'ዳግም', '<UNK>', 'ሰዎች', '<UNK>', '<UNK>'], suggestion: <UNK>\n",
      "Previous tokens: ['ይሁን', 'እና', 'ዳግም', '<UNK>', 'ሰዎች', '<UNK>', '<UNK>', '<UNK>'], suggestion: <UNK>\n",
      "Previous tokens: ['ይሁን', 'እና', 'ዳግም', '<UNK>', 'ሰዎች', '<UNK>', '<UNK>', '<UNK>', 'የሚቆጠሩ'], suggestion: ስደተኞች\n",
      "Previous tokens: ['ይሁን', 'እና', 'ዳግም', '<UNK>', 'ሰዎች', '<UNK>', '<UNK>', '<UNK>', 'የሚቆጠሩ', 'ደግሞ'], suggestion: <UNK>\n",
      "Previous tokens: ['ይሁን', 'እና', 'ዳግም', '<UNK>', 'ሰዎች', '<UNK>', '<UNK>', '<UNK>', 'የሚቆጠሩ', 'ደግሞ', '<UNK>'], suggestion: <UNK>\n",
      "Previous tokens: [], suggestion: <UNK>\n",
      "Previous tokens: ['ስለ'], suggestion: <UNK>\n",
      "Previous tokens: ['ስለ', '<UNK>'], suggestion: <UNK>\n",
      "Previous tokens: ['ስለ', '<UNK>', '<UNK>'], suggestion: <UNK>\n",
      "Previous tokens: ['ስለ', '<UNK>', '<UNK>', 'የብሔራዊ'], suggestion: ፈተና\n",
      "Previous tokens: ['ስለ', '<UNK>', '<UNK>', 'የብሔራዊ', 'አደጋ'], suggestion: <UNK>\n",
      "Previous tokens: ['ስለ', '<UNK>', '<UNK>', 'የብሔራዊ', 'አደጋ', 'ስጋት'], suggestion: <UNK>\n",
      "Previous tokens: ['ስለ', '<UNK>', '<UNK>', 'የብሔራዊ', 'አደጋ', 'ስጋት', '<UNK>'], suggestion: <UNK>\n",
      "Previous tokens: ['ስለ', '<UNK>', '<UNK>', 'የብሔራዊ', 'አደጋ', 'ስጋት', '<UNK>', 'ስራ'], suggestion: <UNK>\n",
      "Previous tokens: ['ስለ', '<UNK>', '<UNK>', 'የብሔራዊ', 'አደጋ', 'ስጋት', '<UNK>', 'ስራ', 'አመራር'], suggestion: <UNK>\n",
      "Previous tokens: ['ስለ', '<UNK>', '<UNK>', 'የብሔራዊ', 'አደጋ', 'ስጋት', '<UNK>', 'ስራ', 'አመራር', '<UNK>'], suggestion: <UNK>\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def main():\n",
    "    n=2\n",
    "    k=1.0\n",
    "    start_token='<ጀ>'\n",
    "    end_token='<ጨ>'\n",
    "    train_tokens, dev_tokens, test_tokens, vocabulary = preprocess_tokens(train_data, dev_data, test_data)\n",
    "    n_gram_counts = count_n_grams(train_tokens, n=n-1)\n",
    "    n_plus1_gram_counts = count_n_grams(train_tokens, n=n)\n",
    "\n",
    "    print('Dev Average Perplexity:', average_perplexity(dev_tokens, n_gram_counts, n_plus1_gram_counts, len(vocabulary), start_token=start_token, end_token=end_token, k=k, n=n))\n",
    "    print('Test Average Perplexity:', average_perplexity(test_tokens, n_gram_counts, n_plus1_gram_counts, len(vocabulary), start_token=start_token, end_token=end_token, k=k, n=n)) \n",
    "\n",
    "    for i, sentence in enumerate(dev_tokens[:11]):\n",
    "        for i in range(len(sentence) - 1):\n",
    "            previous_tokens = sentence[:i]\n",
    "            suggestion = suggest_word(previous_tokens, n_gram_counts, n_plus1_gram_counts, vocabulary, k=k, start_token=start_token, end_token=end_token, n=n)\n",
    "            print(f\"Previous tokens: {previous_tokens}, suggestion: {suggestion}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
