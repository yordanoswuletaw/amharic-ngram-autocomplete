{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re \n",
    "import numpy as np \n",
    "import pandas as pd \n",
    "import matplotlib.pyplot as plt \n",
    "import seaborn as sns\n",
    "from collections import defaultdict\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocess Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5763287 320336 320488\n"
     ]
    }
   ],
   "source": [
    "# read amharic text from a file and return it as a string\n",
    "def load_data(file_path):\n",
    "    # read the file\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        return f.readlines()\n",
    "\n",
    "def consturct_paragraph(text):\n",
    "    paragraph = []\n",
    "    sentence = []\n",
    "    for line in text:\n",
    "        # remove the new line character and the leading and trailing white spaces\n",
    "        word = line.split(' ')[0].strip()\n",
    "        if word:\n",
    "            sentence.append(word)\n",
    "        # check if the word is a ge'ez script delimater\n",
    "        if re.findall(r'(·ç°·ç°|·ç¢|\\?|!|::)', word):\n",
    "            # check if the sentence contains at list two words\n",
    "            if len(sentence) > 1:\n",
    "                paragraph.append(sentence)\n",
    "            sentence = []\n",
    "    return paragraph\n",
    "\n",
    "def tokenize_text(text):\n",
    "    cleaned_text = []\n",
    "    for sentence in text:\n",
    "        punctuation_pattern = r\"[.,;()/\\[\\]{}'\\\"<>@#$%^&*_+=|~\\-]\"\n",
    "        latin_pattern = r'[a-zA-Z0-9]+'\n",
    "        # remove the punctuation marks\n",
    "        cleaned_sentence = re.sub(punctuation_pattern, \"\", sentence)\n",
    "        # remove the latin characters\n",
    "        cleaned_sentence = re.sub(latin_pattern, '', cleaned_sentence)\n",
    "        # remove the extra white spaces\n",
    "        cleaned_sentence = re.sub(r\"\\s+\", \" \", cleaned_sentence).strip()\n",
    "        # check if the sentence contains at list two words\n",
    "        if len(sentence) > 1:\n",
    "            cleaned_text.append(cleaned_sentence)\n",
    "    \n",
    "    tokenized_text = []\n",
    "    sentence = []\n",
    "    # pattern to split the text into words and delimaters\n",
    "    pattern = r'\\w+|[^\\s\\w]+'\n",
    "    for line in cleaned_text:\n",
    "        # split the line into words and delimaters and iterate over the words\n",
    "        for word in re.findall(pattern, line):\n",
    "            # check if the word is a ge'ez script delimater\n",
    "            delimater = re.findall(r'(·ç°·ç°|·ç¢|\\?|!|::)', word)\n",
    "            if delimater:\n",
    "                # add the delimater to the sentence\n",
    "                sentence.append(delimater[-1]) \n",
    "                # add the sentence to the tokenized text\n",
    "                tokenized_text.append(sentence)\n",
    "                sentence = []\n",
    "            elif word:\n",
    "                sentence.append(word)\n",
    "            \n",
    "    return tokenized_text\n",
    "\n",
    "train_data, dev_data, test_data = load_data(\"../data/train.txt\"), load_data(\"../data/dev.txt\"), load_data( \"../data/test.txt\")\n",
    "train_data, dev_data, test_data = consturct_paragraph(train_data), consturct_paragraph(dev_data), consturct_paragraph(test_data)\n",
    "\n",
    "train_txt, dev_txt, test_txt = load_data(\"../data/train_.txt\"), load_data(\"../data/dev_.txt\"), load_data(\"../data/test_.txt\")\n",
    "train_txt = tokenize_text(train_txt)\n",
    "dev_txt = tokenize_text(dev_txt)\n",
    "test_txt = tokenize_text(test_txt)\n",
    "train_data += train_txt\n",
    "dev_data += dev_txt\n",
    "test_data += test_txt\n",
    "print(len(train_data), len(dev_data), len(test_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Proprocess Tokens and Build N-gram Based Langauge Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AmhNgramModel:\n",
    "    def __init__(self, n=2, k=1, threshold=2):\n",
    "        '''\n",
    "        Params:\n",
    "            n: int, the n-gram size\n",
    "            k: int, the k-smoothing parameter\n",
    "            threshold: int, the minimum frequency count of a token to be included in the vocabulary\n",
    "        '''\n",
    "        # n-gram size\n",
    "        self.n = n\n",
    "        # k-smoothing parameter\n",
    "        self.k = k\n",
    "        # threshold for frequency count of tokens\n",
    "        self.threshold = threshold\n",
    "        # tokens frequency count\n",
    "        self.tokens_freq = defaultdict(int)\n",
    "        # n-gram counts\n",
    "        self.n_gram_counts = defaultdict(int)\n",
    "        # (n+1)-gram counts\n",
    "        self.n_plus1_gram_counts = defaultdict(int)\n",
    "        # start token\n",
    "        self.start_token = '<·åÄ>'\n",
    "        # end token\n",
    "        self.end_token = '<·å®>'\n",
    "        # out of vocabulary token \n",
    "        self.oov_token = '<·ä•·äï·åç·ã≥·ç§·âÉ·àç>'\n",
    "        # vocabulary of tokens or words\n",
    "        self.closed_vocabulary = set([self.end_token, self.oov_token])\n",
    "      \n",
    "\n",
    "    def count_tokens_frequency(self,tokenized_sentences):\n",
    "        '''\n",
    "        Count the frequency of tokens in the tokenized sentences\n",
    "        Params:\n",
    "            tokenized_sentences: list of list of strings, tokenized sentences\n",
    "        Returns:\n",
    "            str, completion message\n",
    "        '''\n",
    "        count = 0\n",
    "        for sentence in tokenized_sentences:\n",
    "            for token in sentence:\n",
    "                self.tokens_freq[token] += 1\n",
    "                count += 1\n",
    "        \n",
    "        return f'A total of {count} tokens'\n",
    "\n",
    "    def get_tokens_with_threshold(self):\n",
    "        '''\n",
    "        Get tokens with frequency count greater than or equal to the threshold\n",
    "        Returns: \n",
    "            str, completion message\n",
    "        '''\n",
    "        for token, freq in self.tokens_freq.items():\n",
    "            if freq >= self.threshold:\n",
    "                self.closed_vocabulary.add(token)\n",
    "        \n",
    "        return f'A total of {len(self.closed_vocabulary)} tokens with {self.threshold} threshold'\n",
    "\n",
    "    def replace_oov_tokens(self, tokenized_sentences):\n",
    "        '''\n",
    "        Replace out of vocabulary tokens with a special token\n",
    "        Params:\n",
    "            tokenized_sentences: list of list of strings, tokenized sentences\n",
    "        Returns:\n",
    "            list of list of strings, sentences with out of vocabulary tokens replaced\n",
    "        '''\n",
    "        replaced_sentences = []\n",
    "        for sentence in tokenized_sentences:\n",
    "            # replace out of vocabulary tokens with the oov token\n",
    "            replaced_sentence = [token if token in self.closed_vocabulary else self.oov_token for token in sentence]\n",
    "            replaced_sentences.append(replaced_sentence)\n",
    "        \n",
    "        return replaced_sentences\n",
    "\n",
    "    def preprocess_tokens(self, train_data, dev_data, test_data):\n",
    "        '''\n",
    "        Preprocess the tokenized data\n",
    "        Params:\n",
    "            train_data: list of list of strings, tokenized training data\n",
    "            dev_data: list of list of strings, tokenized development data\n",
    "            test_data: list of list of strings, tokenized test data\n",
    "        Returns:\n",
    "            list of list of strings, preprocessed training data\n",
    "            list of list of strings, preprocessed development data\n",
    "            list of list of strings, preprocessed test data\n",
    "        '''\n",
    "\n",
    "        # count tokens frequency\n",
    "        self.count_tokens_frequency(train_data)\n",
    "        # get tokens with threshold\n",
    "        self.get_tokens_with_threshold()\n",
    "        \n",
    "        # replace out of vocabulary tokens\n",
    "        train_data = self.replace_oov_tokens(train_data)\n",
    "        dev_data = self.replace_oov_tokens(dev_data)\n",
    "        test_data = self.replace_oov_tokens(test_data)\n",
    "\n",
    "        return train_data, dev_data, test_data \n",
    "        \n",
    "\n",
    "    def count_n_grams(self, tokenized_sentences, n):\n",
    "        '''\n",
    "        Count the n-grams in the tokenized sentences\n",
    "        Params:\n",
    "            tokenized_sentences: list of list of strings, tokenized sentences\n",
    "            n: int, the n-gram size\n",
    "        Returns:\n",
    "            dict, n-gram counts\n",
    "        '''\n",
    "        n_grams = defaultdict(int)\n",
    "        for sentence in tokenized_sentences:\n",
    "            # add start and end tokens to the sentence\n",
    "            sentence = [self.start_token] * (n) + sentence + [self.end_token] * (n-1)\n",
    "            for i in range(len(sentence) - n + 1):\n",
    "                # extract n-gram tokens from the sentence\n",
    "                n_gram = tuple(sentence[i:i+n])\n",
    "                n_grams[n_gram] += 1\n",
    "\n",
    "        return n_grams\n",
    "\n",
    "    def estimate_probability(self, word, previous_n_gram):\n",
    "        '''\n",
    "        Estimate the probability of a word given the previous n-gram\n",
    "        Params:\n",
    "            word: str, the word\n",
    "            previous_n_gram: tuple of strings, the previous n-gram\n",
    "        Returns:\n",
    "            float, the probability of the word given the previous n-gram\n",
    "        '''\n",
    "        # convert the previous n-gram to a tuple\n",
    "        previous_n_gram = tuple(previous_n_gram)\n",
    "        # get the count of the previous n-gram\n",
    "        previous_n_gram_count = self.n_gram_counts[previous_n_gram]\n",
    "        # calculate the denominator\n",
    "        denominator = previous_n_gram_count + self.k * len(self.closed_vocabulary)\n",
    "        \n",
    "        # create the n+1 gram\n",
    "        n_plus1_gram = previous_n_gram + (word,)\n",
    "        # get the count of the n+1 gram\n",
    "        n_plus1_gram_count = self.n_plus1_gram_counts[n_plus1_gram]\n",
    "        # calculate the numerator\n",
    "        numerator = n_plus1_gram_count + self.k\n",
    "\n",
    "        # calculate the probability\n",
    "        probability = np.log(numerator) - np.log(denominator)\n",
    "\n",
    "        return probability\n",
    "\n",
    "    def estimate_n_gram_probabilities(self, previous_n_gram):\n",
    "        '''\n",
    "        Estimate the probabilities of the words in the vocabulary given the previous n-gram\n",
    "        Params:\n",
    "            previous_n_gram: tuple of strings, the previous n-gram\n",
    "        Returns:\n",
    "            dict, the probabilities of the words in the vocabulary given the previous n-gram\n",
    "        '''\n",
    "        # convert the previous n-gram to a tuple\n",
    "        previous_n_gram = tuple(previous_n_gram)\n",
    "        # clear the probabilities\n",
    "        probabilities = defaultdict(int)\n",
    "        for word in self.closed_vocabulary:\n",
    "            probability = self.estimate_probability(word, previous_n_gram)\n",
    "            probabilities[word] = probability\n",
    "        \n",
    "        return probabilities\n",
    "    \n",
    "    def suggest_word(self, previous_tokens, top_k_words=1):\n",
    "        '''\n",
    "        Suggest the next word given the previous tokens\n",
    "        Params:\n",
    "            previous_tokens: list of strings, the previous tokens\n",
    "            top_k_words: int, the number of suggestions\n",
    "        Returns:\n",
    "            str, the suggested word\n",
    "        '''\n",
    "        # add start tokens to the previous tokens\n",
    "        previous_tokens = [self.start_token] * (self.n - 1) + previous_tokens\n",
    "        # get the previous n-gram\n",
    "        previous_n_gram = previous_tokens[-(self.n - 1):]\n",
    "        # estimate the probabilities of the words in the vocabulary given the previous n-gram\n",
    "        probabilities = pd.Series(self.estimate_n_gram_probabilities(previous_n_gram))\n",
    "        # get the suggestions\n",
    "        suggestions = probabilities.nlargest(min(probabilities.shape[0], top_k_words))\n",
    "\n",
    "        return suggestions\n",
    "\n",
    "    def fit(self, tokenized_sentences):\n",
    "        '''\n",
    "        Fit the n-gram model to the tokenized sentences\n",
    "        Params:\n",
    "            tokenized_sentences: list of list of strings, tokenized sentences\n",
    "        '''\n",
    "        # compute n-gram counts\n",
    "        self.n_gram_counts = self.count_n_grams(tokenized_sentences, self.n - 1)\n",
    "        # compute (n+1)-gram counts\n",
    "        self.n_plus1_gram_counts = self.count_n_grams(tokenized_sentences, self.n)\n",
    "\n",
    "        return f'üôå'\n",
    "\n",
    "    def calculate_perplexity(self, sentence):\n",
    "        '''\n",
    "        Calculate the perplexity of a sentence\n",
    "        Params:\n",
    "            sentence: list of strings, the sentence\n",
    "        Returns:\n",
    "            float, the perplexity of the sentence\n",
    "        '''\n",
    "        n = n = self.n - 1\n",
    "        # add start and end tokens to the sentence\n",
    "        sentence = [self.start_token] * n + sentence + [self.end_token]\n",
    "        sentence = tuple(sentence)\n",
    "        N = len(sentence)\n",
    "        log_pi = 0\n",
    "        for t in range(n, N):\n",
    "            # get the previous n-gram\n",
    "            n_gram = sentence[t-n:t]\n",
    "            # get the word\n",
    "            word = sentence[t]\n",
    "            # estimate the probability of the word given the previous n-gram\n",
    "            probability = self.estimate_probability(word, n_gram)\n",
    "            # add the log probability to the total log probability\n",
    "            log_pi += probability\n",
    "\n",
    "        # calculate the perplexity\n",
    "        perplexity = np.exp(-log_pi / N)\n",
    "        \n",
    "        return perplexity\n",
    "\n",
    "    def average_perplexity(self, corpus):\n",
    "        '''\n",
    "        Calculate the average perplexity of a corpus\n",
    "        Params:\n",
    "            corpus: list of list of strings, the corpus\n",
    "        Returns:\n",
    "            float, the average perplexity of the corpus\n",
    "        '''\n",
    "        total_perplexity = 0\n",
    "        for sentence in corpus:\n",
    "            # calculate the perplexity of the sentence\n",
    "            sentence_perplexity = self.calculate_perplexity(sentence)\n",
    "            # add the perplexity to the total perplexity\n",
    "            total_perplexity += np.log(sentence_perplexity)\n",
    "\n",
    "        # calculate the average perplexity\n",
    "        return np.exp(total_perplexity / len(corpus))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Amharic Langauge Auto-complete with N-gram langauge model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: <bound method IPythonKernel._clean_thread_parent_frames of <ipykernel.ipkernel.IPythonKernel object at 0x102f84d40>>\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/yo/Documents/amh-auto-complete/venv/lib/python3.12/site-packages/ipykernel/ipkernel.py\", line 775, in _clean_thread_parent_frames\n",
      "    def _clean_thread_parent_frames(\n",
      "\n",
      "KeyboardInterrupt: \n"
     ]
    }
   ],
   "source": [
    "# create an instance of the AmhNgramModel\n",
    "amh_ngram_model = AmhNgramModel(n=2, k=1, threshold=3)\n",
    "# Prprocess the tokenized data\n",
    "train_tokens, dev_tokens, test_tokens = amh_ngram_model.preprocess_tokens(train_data, dev_data, test_data)\n",
    "\n",
    "amh_ngram_model.fit(train_tokens)\n",
    "print('Dev Average Perplexity:', amh_ngram_model.average_perplexity(dev_tokens))\n",
    "print('Test Average Perplexity:', amh_ngram_model.average_perplexity(test_tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Predicting the Next Word for Some Sentences!\n",
      "Word with the square brackets is the suggested word.\n",
      "-------------------------------------------\n",
      "      \n",
      "·â•·àÑ·à©·äï ·ä®·ä¢·âµ·ãÆ·åµ·ã´·ãä·äê·â± ·â†·àã·ã≠ ·ã®·àö·ã´·ã≠ ·ã®·à´·à±·äï ·â•·àÑ·à≠ ·ä®·àå·àã·ãç ·ã≠·â†·àç·å£·àç ·ã®·â†·àà·å† [<·ä•·äï·åç·ã≥·ç§·âÉ·àç>]\n",
      "·ã®·àî·ã∞·äï ·àò·â∞·ãç [·äê·ãç]\n",
      "·àï·åà·ãà·å¶·âΩ [·äì·â∏·ãç]\n",
      "·ã®·ä†·ãç·àÆ·çï·àã·äë ·ã®·ä†·ã∞·åã ·à∞·àà·â£ [·ã®·àÜ·äë]\n",
      "·ä•·äï·âÄ·å•·àç ·àï·åç ·àõ·àà·âµ [·äê·ãç]\n"
     ]
    }
   ],
   "source": [
    "print('''\n",
    "Predicting the Next Word for Some Sentences!\n",
    "Word with the square brackets is the suggested word.\n",
    "-------------------------------------------\n",
    "      ''')\n",
    "\n",
    "for _ in range(5):\n",
    "    # Pick a random sentence from the test set\n",
    "    indx = np.random.randint(0, len(test_tokens))\n",
    "    sentence = dev_tokens[indx]\n",
    "    n = len(sentence) \n",
    "    if n < 2:\n",
    "        continue\n",
    "    # Pick a random suggestion index\n",
    "    suggestion_indx = np.random.randint(1, n)\n",
    "    # Get the previous tokens\n",
    "    previous_tokens = sentence[:suggestion_indx]\n",
    "    # Get the suggestion word\n",
    "    suggestions = amh_ngram_model.suggest_word(previous_tokens, top_k_words=3)\n",
    "    # print the previous tokens and the suggestion\n",
    "    print(suggestions)\n",
    "    print(' '.join(previous_tokens) + ' [' + ']')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
